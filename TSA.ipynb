{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCADA Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import statsmodels.tsa.stattools as sts\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from catboost import CatBoostRegressor\n",
    "import xgboost as xgb\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "from math import sqrt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_attacks = pd.read_csv('data/01_Lev_fault_Temp_corr_seed_11_vars_23.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(data_with_attacks.columns)\n",
    "cols.append(cols.pop(cols.index('ATTACK')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_attacks = data_with_attacks.loc[:,cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_with_attacks[data_with_attacks.ATTACK == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_with_attacks.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for Stationarity\n",
    "\n",
    "The Augmented Dickey Fuller Test (ADF) is unit root test for stationarity. Unit roots can cause unpredictable results in your time series analysis.\n",
    "\n",
    "The hypotheses for the test:\n",
    "* The null hypothesis for this test is that there is a unit root.\n",
    "* The alternate hypothesis differs slightly according to which equation youâ€™re using. The basic alternate is that the time series is stationary (or trend-stationary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "alpha = 0.05\n",
    "\n",
    "for i in range(1,data.shape[1]-1):\n",
    "    test  = sts.adfuller(data.iloc[:,i].diff(1).dropna())    \n",
    "    if(test[1] < alpha):\n",
    "        is_stationary = True\n",
    "    else:\n",
    "        is_stationary = False\n",
    "        \n",
    "    row = {}\n",
    "    row['colname'] = data.columns[i]\n",
    "    row['is_stationary'] = is_stationary\n",
    "    l.append(row)\n",
    "\n",
    "stationarity_tests = pd.DataFrame(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationarity_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationarity_tests.to_csv('output/stationarity_tests.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IQR Anomaly Detection\n",
    "\n",
    "### Calculating range for outliers\n",
    "\n",
    "* The IQR is the length of the box in your box-and-whisker plot. An outlier is any value that lies more than one and a half times the length of the box from either end of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate lower bound\n",
    "def lower_bound(column):\n",
    "    return column.min() - 1.5*(column.quantile(0.75) - column.quantile(0.25))\n",
    "\n",
    "# function to calculate upper bound\n",
    "def upper_bound(column):\n",
    "    return column.max() + 1.5*(column.quantile(0.75) - column.quantile(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "\n",
    "# find lower and upper bound of every column except first and the last\n",
    "for i in range(1, data.shape[1] - 1):\n",
    "    row = {}\n",
    "    row['column'] = data.columns[i]\n",
    "    row['upper_bound'] = data.iloc[:,i].agg(upper_bound)\n",
    "    row['lower_bound'] = data.iloc[:,i].agg(lower_bound)\n",
    "    l.append(row)\n",
    "    \n",
    "iqr_df = pd.DataFrame(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iqr_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iqr_df.to_csv('output/iqr/iqr_test.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing outliers on data with attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to check if outlier lies between lower and upper bounds\n",
    "def iqr_outlier_detection(x):\n",
    "    x = pd.DataFrame(x).reset_index()\n",
    "    x.columns = ['column', 'value']\n",
    "    iqr_x = pd.merge(x, iqr_df, on = 'column', how = 'right')\n",
    "    iqr_x['Test'] = (iqr_x['value'] > iqr_x['upper_bound']) | (iqr_x['value'] < iqr_x['lower_bound'])\n",
    "    return 1 if iqr_x['Test'].any() else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_attacks['iqr_result'] = data_with_attacks.apply(lambda x: iqr_outlier_detection(x), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating IQR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_true = data_with_attacks['ATTACK'], y_pred = data_with_attacks['iqr_result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_true = data_with_attacks['ATTACK'], y_pred = data_with_attacks['iqr_result']) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true = data_with_attacks['ATTACK'], y_pred = data_with_attacks['iqr_result']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(2):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_true = data_with_attacks['ATTACK'], \n",
    "                                  y_score = data_with_attacks['iqr_result'])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true = data_with_attacks['ATTACK'].ravel(), \n",
    "                                          y_score = data_with_attacks['iqr_result'].ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi = 120)\n",
    "lw = 2\n",
    "plt.plot(fpr[1], tpr[1], color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[1])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data for Regression Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = data_with_attacks.drop('Time', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(train_test, test_size=0.25, stratify = train_test['ATTACK'], random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, validation = train_test_split(train, test_size=0.25, stratify=train['ATTACK'], random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = training.drop('ATTACK', axis = 1)\n",
    "y_train = training['ATTACK']\n",
    "x_val = validation.drop('ATTACK', axis = 1)\n",
    "y_val = validation['ATTACK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test.drop('ATTACK', axis = 1)\n",
    "Y_test = test['ATTACK']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing model with parameters\n",
    "cbr = CatBoostRegressor(max_depth=4, \n",
    "                        learning_rate=0.06900,\n",
    "                        loss_function='RMSE',\n",
    "                        n_estimators=500,\n",
    "                        one_hot_max_size=2,\n",
    "                        eval_metric='AUC',\n",
    "                        boosting_type='Ordered',\n",
    "                        random_seed=2405, \n",
    "                        use_best_model=True,\n",
    "                        silent=True,\n",
    "                        random_strength=1.0944250924022183,\n",
    "                        reg_lambda=1.2762795426592872)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbr.fit(x_train, y_train, eval_set=(x_val, y_val),plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Performance of CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = cbr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqrt(mean_squared_error(Y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_prediction = prediction.copy()\n",
    "\n",
    "class_prediction[class_prediction >= 0.5] = 1\n",
    "class_prediction[class_prediction < 0.5] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_true = Y_test, y_pred = class_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_true = Y_test, y_pred = class_prediction) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true = Y_test, y_pred = class_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(2):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_true = Y_test, \n",
    "                                  y_score = class_prediction)\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true = Y_test.ravel(), \n",
    "                                          y_score = class_prediction.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi = 120)\n",
    "lw = 2\n",
    "plt.plot(fpr[1], tpr[1], color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[1])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgc = xgb.XGBClassifier(\n",
    "        n_estimators=10000,\n",
    "        objective='binary:logistic',\n",
    "        n_jobs=-1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(x_train,label=y_train)\n",
    "dtest = xgb.DMatrix(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting parameters for XGBoost\n",
    "parameters={'max_depth':4, \n",
    "            'min_child_weight': 18, \n",
    "            'objective':'binary:logistic',\n",
    "            'eval_metric':'auc',\n",
    "            'learning_rate':0.0638840541928953,\n",
    "            'subsample': 0.7642763922160356,\n",
    "            'colsample_bytree': 0.5695032486938503,\n",
    "            'reg_lambda': 98.64817753125035,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg = xgb.train(parameters,dtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Performance of XBGoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = xg.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance on validation set\n",
    "sqrt(mean_squared_error(y_val, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance on test set\n",
    "dtest_final = xgb.DMatrix(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = xg.predict(dtest_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqrt(mean_squared_error(Y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_prediction = pred.copy()\n",
    "\n",
    "class_prediction[class_prediction >= 0.5] = 1\n",
    "class_prediction[class_prediction < 0.5] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_true = Y_test, y_pred = class_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_true = Y_test, y_pred = class_prediction) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true = Y_test, y_pred = class_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(2):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_true = Y_test, \n",
    "                                  y_score = class_prediction)\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true = Y_test.ravel(), \n",
    "                                          y_score = class_prediction.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi = 120)\n",
    "lw = 2\n",
    "plt.plot(fpr[1], tpr[1], color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[1])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolation Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify the column names to be modelled\n",
    "\n",
    "to_model_columns=data_with_attacks.columns[1:-2]\n",
    "clf=IsolationForest(n_estimators=100, max_samples='auto', contamination=float(.12), \\\n",
    "                        max_features=1.0, bootstrap=False, n_jobs=-1, random_state=42, verbose=0)\n",
    "clf.fit(data_with_attacks[to_model_columns])\n",
    "\n",
    "pred = clf.predict(data_with_attacks[to_model_columns])\n",
    "data_with_attacks['isolation_forest_anomaly']=pred\n",
    "\n",
    "outliers=data_with_attacks.loc[data_with_attacks['isolation_forest_anomaly']==-1]\n",
    "outlier_index=list(outliers.index)\n",
    "\n",
    "#Find the number of anomalies and normal points here points classified -1 are anomalous\n",
    "print(data_with_attacks['isolation_forest_anomaly'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting outliers and inliers using PCA\n",
    "\n",
    "Normalize and fit the metrics to a PCA to reduce the number of dimensions and then plot them in 3D highlighting the anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)  # Reduce to k=3 dimensions\n",
    "scaler = StandardScaler()\n",
    "#normalize the metrics\n",
    "X = scaler.fit_transform(data_with_attacks[to_model_columns])\n",
    "X_reduce = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi = 200)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_zlabel(\"x_composite_3\")\n",
    "# Plot the compressed data points\n",
    "ax.scatter(X_reduce[:, 0], X_reduce[:, 1], zs=X_reduce[:, 2], s=4, lw=1, label=\"inliers\",c=\"green\")\n",
    "# Plot x's for the ground truth outliers\n",
    "ax.scatter(X_reduce[outlier_index,0],X_reduce[outlier_index,1], X_reduce[outlier_index,2],\n",
    "           lw=2, s=60, marker=\"x\", c=\"red\", label=\"outliers\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_attacks['isolation_forest_result'] = data_with_attacks\\\n",
    ".isolation_forest_anomaly.apply(lambda x: 1 if x == -1 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Performance of Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_true = data_with_attacks['ATTACK'], y_pred = data_with_attacks['isolation_forest_result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_true = data_with_attacks['ATTACK'], \n",
    "               y_pred = data_with_attacks['isolation_forest_result'])* 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true = data_with_attacks['ATTACK'], \n",
    "                            y_pred = data_with_attacks['isolation_forest_result']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(2):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_true = data_with_attacks['ATTACK'], \n",
    "                                  y_score = data_with_attacks['isolation_forest_result'])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true = data_with_attacks['ATTACK'].ravel(), \n",
    "                                          y_score = data_with_attacks['isolation_forest_result'].ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi = 120)\n",
    "lw = 2\n",
    "plt.plot(fpr[1], tpr[1], color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[1])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we have figured the anomalous behavior at a use case level.\n",
    "* But to be actionable on the anomaly its important to identify and provide information on which metrics are anomalous in it individually.\n",
    "* The anomalies identified by the algorithm should make sense when viewed visually(sudden dip/peaks) by the business user to act upon it. \n",
    "* So creating a good visualization is equally important in this process.\n",
    "\n",
    "### Individual Metrics\n",
    "\n",
    "* This function creates actuals plot on a time series with anomaly points highlighted on it. Also a table which provides actual data, the change and conditional formatting based on anomalies.\n",
    "* A helper function to find percentage change,classify anomaly based on severity.\n",
    "* The predict function classifies the data as anomalies based on the results from decision function on crossing a threshold.\n",
    "* Say if the business needs to find the next level of anomalies which might have an impact, this could be used to identify those points.\n",
    "* The top 12 quantiles are identified anomalies(high severity), based on decision function here we identify the 12â€“24 quantile points and classify them as low severity anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_anomaly(df, metric_name):\n",
    "    dates = df.Time\n",
    "    # identify the anomaly points and create a array of its values for plot\n",
    "    bool_array = (abs(df['anomaly']) > 0)\n",
    "    actuals = df[\"actuals\"][-len(bool_array):]\n",
    "    anomaly_points = bool_array * actuals\n",
    "    anomaly_points[anomaly_points == 0] = np.nan\n",
    "    # A dictionary for conditional format table based on anomaly\n",
    "    color_map = {0: \"'rgba(228, 222, 249, 0.65)'\", 1: \"yellow\", 2: \"red\"}\n",
    "\n",
    "    # Table which includes Date,Actuals,Change occured from previous point\n",
    "    table = go.Table(\n",
    "        domain=dict(x=[0, 1],\n",
    "                    y=[0, 0.3]),\n",
    "        columnwidth=[1, 2],\n",
    "        # columnorder=[0, 1, 2,],\n",
    "        header=dict(height=20,\n",
    "                    values=[['<b>Date</b>'], ['<b>Actual Values </b>'], ['<b>% Change </b>'],\n",
    "                            ],\n",
    "                    font=dict(color=['rgb(45, 45, 45)'] * 5, size=14),\n",
    "                    fill=dict(color='#d562be')),\n",
    "        cells=dict(values=[df.round(3)[k].tolist() for k in ['Time', 'actuals', 'percentage_change']],\n",
    "                   line=dict(color='#506784'),\n",
    "                   align=['center'] * 5,\n",
    "                   font=dict(color=['rgb(40, 40, 40)'] * 5, size=12),\n",
    "                   # format = [None] + [\",.4f\"] + [',.4f'],\n",
    "                   # suffix=[None] * 4,\n",
    "                   suffix=[None] + [''] + [''] + ['%'] + [''],\n",
    "                   height=27,\n",
    "                   fill=dict(color=[test_df['anomaly_class'].map(color_map)],  # map based on anomaly level from dictionary\n",
    "                             )\n",
    "                   ))\n",
    "    # Plot the actuals points\n",
    "    Actuals = go.Scatter(name='Actuals',\n",
    "                         x=dates,\n",
    "                         y=df['actuals'],\n",
    "                         xaxis='x1', yaxis='y1',\n",
    "                         mode='line',\n",
    "                         marker=dict(size=12,\n",
    "                                     line=dict(width=1),\n",
    "                                     color=\"blue\"))\n",
    "    # Highlight the anomaly points\n",
    "    anomalies_map = go.Scatter(name=\"Anomaly\",\n",
    "                               showlegend=True,\n",
    "                               x=dates,\n",
    "                               y=anomaly_points,\n",
    "                               mode='markers',\n",
    "                               xaxis='x1',\n",
    "                               yaxis='y1',\n",
    "                               marker=dict(color=\"red\",\n",
    "                                           size=11,\n",
    "                                           line=dict(\n",
    "                                               color=\"red\",\n",
    "                                               width=2)))\n",
    "    axis = dict(\n",
    "        showline=True,\n",
    "        zeroline=False,\n",
    "        showgrid=True,\n",
    "        mirror=True,\n",
    "        ticklen=4,\n",
    "        gridcolor='#ffffff',\n",
    "        tickfont=dict(size=10))\n",
    "    layout = dict(\n",
    "        width=1000,\n",
    "        height=865,\n",
    "        autosize=False,\n",
    "        title=metric_name,\n",
    "        margin=dict(t=75),\n",
    "        showlegend=True,\n",
    "        xaxis1=dict(axis, **dict(domain=[0, 1],\n",
    "                                 anchor='y1', showticklabels=True)),\n",
    "        yaxis1=dict(axis, **dict(domain=[2 * 0.21 + 0.20, 1], anchor='x1', hoverformat='.2f')))\n",
    "    fig = go.Figure(data=[table, anomalies_map, Actuals], layout=layout)\n",
    "    plot(fig, filename='output/isolation_forest/{0}.html'.format(metric_name), auto_open=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_anomalies(df, metric_name):\n",
    "    df['metric_name'] = metric_name\n",
    "    df = df.sort_values(by='Time', ascending=False)\n",
    "    # Shift actuals by one Time to find the percentage chage between current and previous data point\n",
    "    df['shift'] = df['actuals'].shift(-1)\n",
    "    df['percentage_change'] = (\n",
    "        (df['actuals'] - df['shift']) / df['actuals']) * 100\n",
    "    # Categorise anomalies as 0-no anomaly, 1- low anomaly , 2 - high anomaly\n",
    "    df['anomaly'].loc[df['anomaly'] == 1] = 0\n",
    "    df['anomaly'].loc[df['anomaly'] == -1] = 2\n",
    "    df['anomaly_class'] = df['anomaly']\n",
    "    max_anomaly_score = df['score'].loc[df['anomaly_class'] == 2].max()\n",
    "    medium_percentile = df['score'].quantile(0.24)\n",
    "    df['anomaly_class'].loc[(df['score'] > max_anomaly_score) & (\n",
    "        df['score'] <= medium_percentile)] = 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plots for each individual metrics saved as HTML file in plots folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, len(data_with_attacks.columns)-4):\n",
    "    clf.fit(data_with_attacks.iloc[:, i:i+1])\n",
    "    pred = clf.predict(data_with_attacks.iloc[:, i:i+1])\n",
    "    test_df = pd.DataFrame()\n",
    "    test_df['Time'] = data_with_attacks['Time']\n",
    "    \n",
    "    # Find decision function to find the score and classify anomalies\n",
    "    test_df['score'] = clf.decision_function(data_with_attacks.iloc[:, i:i+1])\n",
    "    test_df['actuals'] = data_with_attacks.iloc[:, i:i+1]\n",
    "    test_df['anomaly'] = pred\n",
    "    \n",
    "    test_df = classify_anomalies(test_df, data_with_attacks.columns[i])\n",
    "    plot_anomaly(test_df, data_with_attacks.columns[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphical Representation of IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_iqr_anomaly(df, metric_name):\n",
    "    dates = df.Time\n",
    "    # identify the anomaly points and create a array of its values for plot\n",
    "    bool_array = (abs(df['anomaly']) > 0)\n",
    "    actuals = df[\"actuals\"][-len(bool_array):]\n",
    "    anomaly_points = bool_array * actuals\n",
    "    anomaly_points[anomaly_points == 0] = np.nan\n",
    "    # A dictionary for conditional format table based on anomaly\n",
    "    color_map = {0: \"'rgba(228, 222, 249, 0.65)'\", 1: \"red\"}\n",
    "\n",
    "    # Table which includes Date,Actuals,Change occured from previous point\n",
    "    table = go.Table(\n",
    "        domain=dict(x=[0, 1],\n",
    "                    y=[0, 0.3]),\n",
    "        columnwidth=[1, 2],\n",
    "        # columnorder=[0, 1, 2,],\n",
    "        header=dict(height=20,\n",
    "                    values=[['<b>Date</b>'], ['<b>Actual Values </b>'], ['<b>% Change </b>'],\n",
    "                            ],\n",
    "                    font=dict(color=['rgb(45, 45, 45)'] * 5, size=14),\n",
    "                    fill=dict(color='#d562be')),\n",
    "        cells=dict(values=[df.round(3)[k].tolist() for k in ['Time', 'actuals', 'percentage_change']],\n",
    "                   line=dict(color='#506784'),\n",
    "                   align=['center'] * 5,\n",
    "                   font=dict(color=['rgb(40, 40, 40)'] * 5, size=12),\n",
    "                   # format = [None] + [\",.4f\"] + [',.4f'],\n",
    "                   # suffix=[None] * 4,\n",
    "                   suffix=[None] + [''] + [''] + ['%'] + [''],\n",
    "                   height=27,\n",
    "                   fill=dict(color=[test_df['anomaly_class'].map(color_map)],  # map based on anomaly level from dictionary\n",
    "                             )\n",
    "                   ))\n",
    "    # Plot the actuals points\n",
    "    Actuals = go.Scatter(name='Actuals',\n",
    "                         x=dates,\n",
    "                         y=df['actuals'],\n",
    "                         xaxis='x1', yaxis='y1',\n",
    "                         mode='line',\n",
    "                         marker=dict(size=12,\n",
    "                                     line=dict(width=1),\n",
    "                                     color=\"blue\"))\n",
    "    # Highlight the anomaly points\n",
    "    anomalies_map = go.Scatter(name=\"Anomaly\",\n",
    "                               showlegend=True,\n",
    "                               x=dates,\n",
    "                               y=anomaly_points,\n",
    "                               mode='markers',\n",
    "                               xaxis='x1',\n",
    "                               yaxis='y1',\n",
    "                               marker=dict(color=\"red\",\n",
    "                                           size=11,\n",
    "                                           line=dict(\n",
    "                                               color=\"red\",\n",
    "                                               width=2)))\n",
    "    axis = dict(\n",
    "        showline=True,\n",
    "        zeroline=False,\n",
    "        showgrid=True,\n",
    "        mirror=True,\n",
    "        ticklen=4,\n",
    "        gridcolor='#ffffff',\n",
    "        tickfont=dict(size=10))\n",
    "    layout = dict(\n",
    "        width=1000,\n",
    "        height=865,\n",
    "        autosize=False,\n",
    "        title=metric_name,\n",
    "        margin=dict(t=75),\n",
    "        showlegend=True,\n",
    "        xaxis1=dict(axis, **dict(domain=[0, 1],\n",
    "                                 anchor='y1', showticklabels=True)),\n",
    "        yaxis1=dict(axis, **dict(domain=[2 * 0.21 + 0.20, 1], anchor='x1', hoverformat='.2f')))\n",
    "    fig = go.Figure(data=[table, anomalies_map, Actuals], layout=layout)\n",
    "    plot(fig, filename='output/iqr/{0}.html'.format(metric_name), auto_open=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_iqr_anomalies(df, metric_name):\n",
    "    df['metric_name'] = metric_name\n",
    "    df = df.sort_values(by='Time', ascending=False)\n",
    "    # Shift actuals by one Time to find the percentage chage between current and previous data point\n",
    "    df['shift'] = df['actuals'].shift(-1)\n",
    "    df['percentage_change'] = (\n",
    "        (df['actuals'] - df['shift']) / df['actuals']) * 100\n",
    "    \n",
    "    # Categorise anomalies as 0-no anomaly, 1 - high anomaly\n",
    "    df['anomaly_class'] = df['anomaly']\n",
    "    max_anomaly_score = df['score'].loc[df['anomaly_class'] == 1].max()\n",
    "    medium_percentile = df['score'].quantile(0.24)\n",
    "    df['anomaly_class'].loc[(df['score'] > max_anomaly_score) & (\n",
    "        df['score'] <= medium_percentile)] = 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to check if outlier lies between lower and upper bounds\n",
    "def iqr_outlier_detection_col(x, colname):\n",
    "    res = (x > iqr_df[iqr_df.column == colname]['upper_bound']) | (x < iqr_df[iqr_df.column == colname]['lower_bound'])\n",
    "    return 1 if res.iloc[0] else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to check if drift outside upper and lower bounds\n",
    "def iqr_score(x, colname):\n",
    "    row = iqr_df[iqr_df.column == colname]\n",
    "    if(x > row['upper_bound'].iloc[0]):\n",
    "        return (x - row['upper_bound']).iloc[0]\n",
    "    elif(x < row['lower_bound'].iloc[0]):\n",
    "        return (row['lower_bound'] - x).iloc[0]\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, len(data_with_attacks.columns)-4):\n",
    "    pred = data_with_attacks.iloc[:, i]\\\n",
    "    .apply(lambda x: iqr_outlier_detection_col(x, data_with_attacks.columns[i]))\n",
    "    test_df = pd.DataFrame()\n",
    "    test_df['Time'] = data_with_attacks['Time']\n",
    "    \n",
    "    # Find decision function to find the score and classify anomalies\n",
    "    test_df['score'] = data_with_attacks.iloc[:, i]\\\n",
    "    .apply(lambda x: iqr_score(x, data_with_attacks.columns[i]))\n",
    "    test_df['actuals'] = data_with_attacks.iloc[:, i]\n",
    "    test_df['anomaly'] = pred\n",
    "    \n",
    "    test_df = classify_iqr_anomalies(test_df, data_with_attacks.columns[i])\n",
    "    plot_iqr_anomaly(test_df, data_with_attacks.columns[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. https://towardsdatascience.com/anomaly-detection-with-isolation-forest-visualization-23cd75c281e2\n",
    "2. https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
